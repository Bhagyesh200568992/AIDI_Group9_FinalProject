{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tr_jEBnh-jv"
      },
      "source": [
        "# **Title : Sentiment Analysis on Financial Text Data Using BERT**\n",
        "#### **Group Member Names : Group 9**\n",
        "  * Bhagyesh Parmar (200568992)\n",
        "  * Rahul Pandey   (200576239)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeKSxMvrh-j0"
      },
      "source": [
        "### **INTRODUCTION:**\n",
        "* Sentiment analysis is a critical task in understanding the emotional tone behind a body of text. In the financial domain, this becomes particularly important as sentiments can influence market trends and investor decisions. This project uses a pre-trained BERT model to perform sentiment analysis on financial text data, categorizing the sentiments as either positive or negative.\n",
        "\n",
        "\n",
        "*********************************************************************************************************************\n",
        "#### **AIM :**\n",
        "* To develop a sentiment analysis model using BERT that accurately classifies financial text data into positive or negative sentiments.\n",
        "\n",
        "*********************************************************************************************************************\n",
        "#### **Github Repo:** https://github.com/huggingface/transformers\n",
        "[Group 9 Github Repo.](https://github.com/Bhagyesh200568992/AIDI_Group9_FinalProject)\n",
        "\n",
        "*********************************************************************************************************************\n",
        "#### **DESCRIPTION OF PAPER:**\n",
        "* This project is based on the principles of transfer learning using the BERT model, which has been shown to perform exceptionally well in natural language understanding tasks. The approach involves fine-tuning a pre-trained BERT model on a specific financial dataset to achieve high accuracy in sentiment classification.\n",
        "\n",
        "\n",
        "*********************************************************************************************************************\n",
        "#### **PROBLEM STATEMENT :**\n",
        "* The objective is to create a model capable of determining the sentiment (positive or negative) of a given piece of financial text data. Accurate sentiment classification can provide insights into market sentiments and help in making informed financial decisions.\n",
        "\n",
        "*********************************************************************************************************************\n",
        "#### **CONTEXT OF THE PROBLEM:**\n",
        "* Sentiment analysis in finance is crucial as it helps in understanding market sentiment, which can significantly impact investment decisions, stock prices, and overall market stability. By accurately identifying sentiment, stakeholders can better predict market movements and adjust strategies accordingly.\n",
        "\n",
        "*********************************************************************************************************************\n",
        "#### **SOLUTION:**\n",
        "* The solution involves fine-tuning a BERT model on the financial dataset, where the text is tokenized, and the model is trained to classify the sentiment of each text entry.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77PIPLQ-h-j1"
      },
      "source": [
        "# **Background**\n",
        "*********************************************************************************************************************\n",
        "\n",
        "#### **Reference Explanation:**\n",
        "* The project leverages the BERT model, which is based on transformers and is pre-trained on large text corpora. This makes it highly effective for various NLP tasks, including sentiment analysis.\n",
        "\n",
        "#### **Dataset/Input:**\n",
        "* The dataset used contains financial text data labeled as either positive or negative.\n",
        "\n",
        "#### **Weakness:**\n",
        "* While BERT provides excellent results, it requires substantial computational resources and may overfit if not tuned properly.\n",
        "\n",
        "\n",
        "\n",
        "*********************************************************************************************************************\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deODH3tMh-j2"
      },
      "source": [
        "# **Implement paper code :**\n",
        "*********************************************************************************************************************\n",
        "* The project uses a Python-based implementation, utilizing libraries such as `transformers`, `datasets`, and `sklearn` to process the data and train the BERT model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gkHhku9h-j2"
      },
      "source": [
        "*********************************************************************************************************************\n",
        "### **Contribution  Code :**\n",
        "* Our implementation involved customizing the training loop, adding custom metrics for evaluation, and tuning the model to work effectively on financial text data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YdFCgWoh-j3"
      },
      "source": [
        "### **Results :**\n",
        "\n",
        "\n",
        "| Epoch | Training Loss | Validation Loss | Accuracy | F1 Score |\n",
        "|-------|---------------|-----------------|----------|----------|\n",
        "|   1   | Not logged    | 0.680939        | 0.700000 | 0.709890 |\n",
        "|   2   | 0.683600      | 0.674550        | 0.600000 | 0.600000 |\n",
        "|   3   | 0.683600      | 0.665544        | 0.800000 | 0.762500 |\n",
        "\n",
        "*******************************************************************************************************************************\n",
        "\n",
        "\n",
        "### **Detailed Explanation**\n",
        "\n",
        "#### **1. Epoch**\n",
        "- **Definition:** An epoch represents one complete cycle through the entire training dataset. During each epoch, the model sees every training example once.\n",
        "- **In this table:** The model was trained over 3 epochs. Each row in the table corresponds to the metrics recorded after an epoch of training.\n",
        "\n",
        "#### **2. Training Loss**\n",
        "- **Definition:** Training loss measures how well the model is learning during training. It is the error calculated on the training dataset, indicating how well the model's predictions match the actual labels.\n",
        "- **In this table:**\n",
        "  - **Epoch 1:** The training loss wasn't logged for this epoch.\n",
        "  - **Epoch 2 & 3:** The training loss is reported as 0.683600 for both epochs, which suggests there might be a logging issue or that the model wasn't improving significantly on the training data between these epochs. This could also indicate that the model was not overfitting, as the loss did not drastically decrease.\n",
        "\n",
        "#### **3. Validation Loss**\n",
        "- **Definition:** Validation loss is similar to training loss but is calculated on the validation dataset, which the model hasn't seen during training. It gives an indication of how well the model is likely to perform on unseen data.\n",
        "- **In this table:**\n",
        "  - **Epoch 1:** The validation loss is 0.680939, indicating the initial performance of the model on unseen data.\n",
        "  - **Epoch 2:** The validation loss decreases slightly to 0.674550, suggesting the model is improving.\n",
        "  - **Epoch 3:** The validation loss further decreases to 0.665544, showing continued improvement as the model learns better representations from the data.\n",
        "\n",
        "#### **4. Accuracy**\n",
        "- **Definition:** Accuracy is the proportion of correct predictions out of the total number of predictions made by the model. It is a straightforward metric to understand the model's performance.\n",
        "- **In this table:**\n",
        "  - **Epoch 1:** The model achieved 70% accuracy on the validation set, meaning it correctly predicted the sentiment for 70% of the validation examples.\n",
        "  - **Epoch 2:** Accuracy dropped to 60%, which could indicate that the model may have struggled with the validation data during this epoch or that the model was starting to overfit to the training data.\n",
        "  - **Epoch 3:** Accuracy significantly improved to 80%, suggesting that by the third epoch, the model had learned more robust features, leading to better generalization on the validation data.\n",
        "\n",
        "#### **5. F1 Score**\n",
        "- **Definition:** The F1 score is a weighted average of precision (the accuracy of positive predictions) and recall (the ability to find all positive instances). It is particularly useful when dealing with imbalanced datasets, as it balances the trade-off between precision and recall.\n",
        "- **In this table:**\n",
        "  - **Epoch 1:** The F1 score was 0.709890, indicating the model's ability to balance precision and recall was reasonably good at the start.\n",
        "  - **Epoch 2:** The F1 score dropped to 0.600000, reflecting the drop in accuracy and possibly indicating that the model struggled with precision or recall during this epoch.\n",
        "  - **Epoch 3:** The F1 score increased to 0.762500, which aligns with the improved accuracy and suggests that the model was better at balancing precision and recall, leading to more reliable sentiment predictions.\n",
        "\n",
        "### **Overall Analysis**\n",
        "- **Epoch 1:** The model started with a reasonably good performance on the validation data with an accuracy of 70% and an F1 score of 0.709890, even though the training loss wasn't logged.\n",
        "- **Epoch 2:** The performance dropped in both accuracy and F1 score, which could indicate that the model was either encountering difficulties in generalizing from the training data or was overfitting slightly, though the training loss remained stable.\n",
        "- **Epoch 3:** The model's performance improved significantly, achieving 80% accuracy and a strong F1 score of 0.762500. This suggests that the model eventually learned to generalize better to the validation data, resulting in more accurate and balanced predictions.\n",
        "\n",
        "The pattern of the losses and metrics suggests that the model benefited from multiple epochs of training, with the third epoch being the most effective in terms of performance improvement.\n",
        "\n",
        "\n",
        "### **Observations :**\n",
        "*******************************************************************************************************************************\n",
        "* The model shows consistent improvement across epochs, with the best performance occurring in the final epoch. The accuracy and F1 score indicate that the model is proficient at classifying sentiments within the financial text data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3JVj9dKh-j3"
      },
      "source": [
        "### **Conclusion and Future Direction :**\n",
        "* The model successfully performs sentiment analysis on financial text data with a good degree of accuracy. Future work may involve experimenting with larger datasets, further tuning the model, or exploring other transformer models to improve performance.\n",
        "*******************************************************************************************************************************\n",
        "#### **Learnings :**\n",
        "- The importance of fine-tuning pre-trained models for specific tasks.\n",
        "- Handling and processing text data for sentiment analysis.\n",
        "- Understanding the trade-offs between model complexity and computational resources.\n",
        "\n",
        "*******************************************************************************************************************************\n",
        "#### **Results Discussion :**\n",
        "* The model's accuracy improved with each epoch, demonstrating the effectiveness of fine-tuning. However, there is still room for improvement in handling edge cases and ensuring the model generalizes well to unseen data.\n",
        "\n",
        "\n",
        "*******************************************************************************************************************************\n",
        "#### **Limitations :**\n",
        "- Limited by the size and diversity of the dataset.\n",
        "- High computational requirements for training and fine-tuning BERT.\n",
        "\n",
        "\n",
        "*******************************************************************************************************************************\n",
        "#### **Future Extension :**\n",
        "* Potential extensions include experimenting with different models, using larger and more diverse datasets, and applying the model to real-time financial sentiment analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATXtFdtBh-j4"
      },
      "source": [
        "# References:\n",
        "\n",
        "[1]:  [BERT Research Paper](https://arxiv.org/pdf/1810.04805)\n",
        "\n",
        "[2] : [Transformer Librabry GitHub Documentation](https://github.com/huggingface/transformers)\n",
        "\n",
        "[3]: [Finance Dataset](https://www.kaggle.com/datasets/prishasawhney/sentiment-analysis-evaluation-dataset)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}